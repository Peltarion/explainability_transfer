## BART for generative explanations
This subrepo is used to train a BART model for extractive explanation generation similar to the WT5 framework. It is based on the previous [seq2seq examples using PyTorch Lightning in the Huggingface transformers library](https://github.com/huggingface/transformers/tree/8f07f5c44bf33f10b0075ce770b19de96ab389c0/examples/seq2seq).

Exemplary training scripts that illustrates how to perform explainability transfer, train baseline models etc. can be found in `explainability_transfer/scripts/bart`.
